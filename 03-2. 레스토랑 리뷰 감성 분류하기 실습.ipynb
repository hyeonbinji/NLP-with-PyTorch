{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "427ab638",
   "metadata": {},
   "outputs": [],
   "source": [
    "#옐프 리뷰 데이터를 위한 파이토치 데이터 클래스"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f767a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "from argparse import Namespace"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "673fc500",
   "metadata": {},
   "source": [
    "# 데이터 전처리"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c472b7a8",
   "metadata": {},
   "source": [
    "#### 데이터 전처리 과정에서 사용할 변수 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e68e342e",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Namespace(\n",
    "    raw_train_dataset_csv=\"/Users/jihyeonbin/raw_train.csv\",\n",
    "    raw_test_dataset_csv=\"/Users/jihyeonbin//raw_test.csv\",\n",
    "    proportion_subset_of_train=0.1,\n",
    "    train_proportion=0.7,\n",
    "    val_proportion=0.15,\n",
    "    test_proportion=0.15,\n",
    "    output_munged_csv=\"/Users/jihyeonbin/reviews_with_splits_lite.csv\",\n",
    "    seed=1337\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fef26768",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rating</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Unfortunately, the frustration of being Dr. Go...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Been going to Dr. Goldberg for over 10 years. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>I don't know what Dr. Goldberg was like before...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>I'm writing this review to give you a heads up...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>All the food is great here. But the best thing...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   rating                                             review\n",
       "0       1  Unfortunately, the frustration of being Dr. Go...\n",
       "1       2  Been going to Dr. Goldberg for over 10 years. ...\n",
       "2       1  I don't know what Dr. Goldberg was like before...\n",
       "3       1  I'm writing this review to give you a heads up...\n",
       "4       2  All the food is great here. But the best thing..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#원본 데이터 읽기\n",
    "train_reviews = pd.read_csv(args.raw_train_dataset_csv, header=None, names=['rating', 'review'])\n",
    "train_reviews.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "295d4032",
   "metadata": {},
   "outputs": [],
   "source": [
    "#클래스별로 리뷰 비율 동일하게 만든다.\n",
    "by_rating = collections.defaultdict(list)\n",
    "for _, row in train_reviews.iterrows(): #iterrows:데이터프레임의 행 반복하기\n",
    "    by_rating[row.rating].append(row.to_dict())\n",
    "    \n",
    "review_subset = []\n",
    "\n",
    "for _, item_list in sorted(by_rating.items()):\n",
    "\n",
    "    n_total = len(item_list)\n",
    "    n_subset = int(args.proportion_subset_of_train * n_total) #10퍼센트만 사용하도록\n",
    "    review_subset.extend(item_list[:n_subset])\n",
    "\n",
    "review_subset = pd.DataFrame(review_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3bec5c58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rating</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Unfortunately, the frustration of being Dr. Go...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>I don't know what Dr. Goldberg was like before...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>I'm writing this review to give you a heads up...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>Wing sauce is like water. Pretty much a lot of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>Owning a driving range inside the city limits ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   rating                                             review\n",
       "0       1  Unfortunately, the frustration of being Dr. Go...\n",
       "1       1  I don't know what Dr. Goldberg was like before...\n",
       "2       1  I'm writing this review to give you a heads up...\n",
       "3       1  Wing sauce is like water. Pretty much a lot of...\n",
       "4       1  Owning a driving range inside the city limits ..."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_subset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "99e6b281",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    280000\n",
       "2    280000\n",
       "Name: rating, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_reviews.rating.value_counts() #총 56만개"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "83b595d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1, 2}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(review_subset.rating)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cf7a479",
   "metadata": {},
   "source": [
    "1: negative, 2: postivie"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8137ae8",
   "metadata": {},
   "source": [
    "#### 훈련, 검증, 테스트 데이터 만들기: 별점 기준으로 나누어서\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e6a5c679",
   "metadata": {},
   "outputs": [],
   "source": [
    "by_rating = collections.defaultdict(list)\n",
    "for _, row in review_subset.iterrows():\n",
    "    by_rating[row.rating].append(row.to_dict())\n",
    "\n",
    "# 분할 데이터만들기\n",
    "final_list = []\n",
    "np.random.seed(args.seed)\n",
    "\n",
    "for _, item_list in sorted(by_rating.items()):\n",
    "\n",
    "    np.random.shuffle(item_list)\n",
    "    \n",
    "    n_total = len(item_list)\n",
    "    n_train = int(args.train_proportion * n_total)\n",
    "    n_val = int(args.val_proportion * n_total)\n",
    "    n_test = int(args.test_proportion * n_total)\n",
    "    \n",
    "    # 데이터 포인터에 분할 속성을 추가\n",
    "    for item in item_list[:n_train]:\n",
    "        item['split'] = 'train'\n",
    "    \n",
    "    for item in item_list[n_train:n_train+n_val]:\n",
    "        item['split'] = 'val'\n",
    "        \n",
    "    for item in item_list[n_train+n_val:n_train+n_val+n_test]:\n",
    "        item['split'] = 'test'\n",
    "\n",
    "    # 최종 리스트에 추가\n",
    "    final_list.extend(item_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b007bf86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rating</th>\n",
       "      <th>review</th>\n",
       "      <th>split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Terrible place to work for I just heard a stor...</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3 hours, 15 minutes-- total time for an extrem...</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>My less than stellar review is for service.   ...</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>I'm granting one star because there's no way t...</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>The food here is mediocre at best. I went afte...</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   rating                                             review  split\n",
       "0       1  Terrible place to work for I just heard a stor...  train\n",
       "1       1  3 hours, 15 minutes-- total time for an extrem...  train\n",
       "2       1  My less than stellar review is for service.   ...  train\n",
       "3       1  I'm granting one star because there's no way t...  train\n",
       "4       1  The food here is mediocre at best. I went afte...  train"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_reviews = pd.DataFrame(final_list)\n",
    "final_reviews.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bc1d414e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "train    39200\n",
       "test      8400\n",
       "val       8400\n",
       "Name: split, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_reviews.split.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "920c5f3d",
   "metadata": {},
   "source": [
    "#### 데이터 정제하기: 공백을 기준으로 토큰을 나누고 구두점(쉼포, 마침표) 기호 앞뒤에 공백 넣고 다른 기호는 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "009bcfb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"([.,!?])\", r\" \\1 \", text)\n",
    "    text = re.sub(r\"[^a-zA-Z.,!?]+\", r\" \", text)\n",
    "    return text\n",
    "    \n",
    "final_reviews.review = final_reviews.review.apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d01fd620",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rating</th>\n",
       "      <th>review</th>\n",
       "      <th>split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>negative</td>\n",
       "      <td>terrible place to work for i just heard a stor...</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>negative</td>\n",
       "      <td>hours , minutes total time for an extremely s...</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>negative</td>\n",
       "      <td>my less than stellar review is for service . w...</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>negative</td>\n",
       "      <td>i m granting one star because there s no way t...</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>negative</td>\n",
       "      <td>the food here is mediocre at best . i went aft...</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     rating                                             review  split\n",
       "0  negative  terrible place to work for i just heard a stor...  train\n",
       "1  negative   hours , minutes total time for an extremely s...  train\n",
       "2  negative  my less than stellar review is for service . w...  train\n",
       "3  negative  i m granting one star because there s no way t...  train\n",
       "4  negative  the food here is mediocre at best . i went aft...  train"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#숫자 1,2를 각각 negative, positive로 변환\n",
    "final_reviews['rating'] = final_reviews.rating.apply({1: 'negative', 2: 'positive'}.get)\n",
    "final_reviews.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cf9a1879",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_reviews.to_csv(args.output_munged_csv, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d1cbd09",
   "metadata": {},
   "source": [
    "## 문장 토큰화와 관련 클래스 객체"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6f0f9d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "from argparse import Namespace\n",
    "from collections import Counter\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import string\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "756e5e19",
   "metadata": {},
   "source": [
    "### >> 파이토치 데이터셋 이해하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "971c0a9d",
   "metadata": {},
   "source": [
    "파이토치는 Dataset 클래스로 데이터셋 추상화한다.\n",
    "\n",
    "새로운 데이터셋 사용 위해서 Dataset 클래스를 상속하여 getitem()과 len() 구현해야 한다.\n",
    "\n",
    "DataLoader + ReviewVectorizer 파이토치 유틸리티 다수 사용\n",
    "\n",
    "ReviewVectorizer: 리뷰 텍스트를 수치 데이터로 변환 >> 변환 후 신경망이 데이터 다룰 수 있다.\n",
    "    \n",
    "전체적인 설계: 1. 데이터 포인트 하나에 벡터화 로직 적용하는 데이터셋 클래스 구현 \n",
    "2. 파이토치 DataLoader 클래스가 데이터셋에서 샘플링하고 모아서 미니배치 만든다"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34150126",
   "metadata": {},
   "source": [
    "# 데이터 벡터화 클래스"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fbfed6d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ReviewVectorizer는 리뷰 데이터를 수치 벡터로 변환하는 클래스\n",
    "class ReviewDataset(Dataset): #dataset 상속\n",
    "    def __init__(self, review_df, vectorizer):\n",
    "        \"\"\"\n",
    "        매개변수:\n",
    "            review_df (pandas.DataFrame): 데이터셋\n",
    "            vectorizer (ReviewVectorizer): ReviewVectorizer 객체\n",
    "        \"\"\"\n",
    "        self.review_df = review_df\n",
    "        self._vectorizer = vectorizer\n",
    "\n",
    "        self.train_df = self.review_df[self.review_df.split=='train']\n",
    "        self.train_size = len(self.train_df)\n",
    "\n",
    "        self.val_df = self.review_df[self.review_df.split=='val']\n",
    "        self.validation_size = len(self.val_df)\n",
    "\n",
    "        self.test_df = self.review_df[self.review_df.split=='test']\n",
    "        self.test_size = len(self.test_df)\n",
    "\n",
    "        self._lookup_dict = {'train': (self.train_df, self.train_size),\n",
    "                             'val': (self.val_df, self.validation_size),\n",
    "                             'test': (self.test_df, self.test_size)}\n",
    "\n",
    "        self.set_split('train')\n",
    "\n",
    "    @classmethod\n",
    "    #classmethod: 객체(인스턴스)를 만들지 않아도 class의 메서드 바로 실행 가능\n",
    "    #cls인자는 클래스의 어떤 속성에도 접근할 수 있다. >> ReviewDataset 클래스를 뜻한다.\n",
    "    def load_dataset_and_make_vectorizer(cls, review_csv):\n",
    "        \"\"\" 데이터셋을 로드하고 새로운 ReviewVectorizer 객체를 만듭니다\n",
    "        \n",
    "        매개변수:\n",
    "            review_csv (str): 데이터셋의 위치\n",
    "        반환값:\n",
    "            ReviewDataset의 인스턴스\n",
    "        \"\"\"\n",
    "        review_df = pd.read_csv(review_csv)#정제된 데이터 가져올거다\n",
    "        train_review_df = review_df[review_df.split=='train']\n",
    "        return cls(review_df, ReviewVectorizer.from_dataframe(train_review_df))\n",
    "    \n",
    "    @classmethod\n",
    "    def load_dataset_and_load_vectorizer(cls, review_csv, vectorizer_filepath):\n",
    "        \"\"\" 데이터셋을 로드하고 새로운 ReviewVectorizer 객체를 만듭니다.\n",
    "        캐시된 ReviewVectorizer 객체를 재사용할 때 사용합니다.\n",
    "        \n",
    "        매개변수:\n",
    "            review_csv (str): 데이터셋의 위치\n",
    "            vectorizer_filepath (str): ReviewVectorizer 객체의 저장 위치\n",
    "        반환값:\n",
    "            ReviewDataset의 인스턴스\n",
    "        \"\"\"\n",
    "        review_df = pd.read_csv(review_csv)\n",
    "        vectorizer = cls.load_vectorizer_only(vectorizer_filepath)\n",
    "        return cls(review_df, vectorizer)\n",
    "\n",
    "    @staticmethod\n",
    "    def load_vectorizer_only(vectorizer_filepath):\n",
    "        \"\"\" 파일에서 ReviewVectorizer 객체를 로드하는 정적 메서드\n",
    "        \n",
    "        매개변수:\n",
    "            vectorizer_filepath (str): 직렬화된 ReviewVectorizer 객체의 위치\n",
    "        반환값:\n",
    "            ReviewVectorizer의 인스턴스\n",
    "        \"\"\"\n",
    "        with open(vectorizer_filepath) as fp:\n",
    "            return ReviewVectorizer.from_serializable(json.load(fp))\n",
    "        #json파일에서 reviewvecotrizer 만들기\n",
    "\n",
    "    def save_vectorizer(self, vectorizer_filepath):\n",
    "        \"\"\" ReviewVectorizer 객체를 json 형태로 디스크에 저장합니다\n",
    "        \n",
    "        매개변수\n",
    "            vectorizer_filepath (str): ReviewVectorizer 객체의 저장 위치\n",
    "        \"\"\"\n",
    "        with open(vectorizer_filepath, \"w\") as fp:\n",
    "            json.dump(self._vectorizer.to_serializable(), fp)\n",
    "\n",
    "    def get_vectorizer(self):\n",
    "        \"\"\" 벡터 변환 객체를 반환합니다 \"\"\"\n",
    "        return self._vectorizer\n",
    "\n",
    "    def set_split(self, split=\"train\"):\n",
    "        \"\"\" 데이터프레임에 있는 열을 사용해 분할 세트를 선택합니다 \n",
    "        \n",
    "        매개변수:\n",
    "            split (str): \"train\", \"val\", \"test\" 중 하나\n",
    "        \"\"\"\n",
    "        self._target_split = split\n",
    "        self._target_df, self._target_size = self._lookup_dict[split]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self._target_size\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\" 파이토치 데이터셋의 주요 진입 메서드\n",
    "        \n",
    "        매개변수:\n",
    "            index (int): 데이터 포인트의 인덱스\n",
    "        반환값:\n",
    "            데이터 포인트의 특성(x_data)과 레이블(y_target)로 이루어진 딕셔너리\n",
    "        \"\"\"\n",
    "        row = self._target_df.iloc[index]\n",
    "\n",
    "        review_vector = \\\n",
    "            self._vectorizer.vectorize(row.review)\n",
    "\n",
    "        rating_index = \\\n",
    "            self._vectorizer.rating_vocab.lookup_token(row.rating)\n",
    "\n",
    "        return {'x_data': review_vector,\n",
    "                'y_target': rating_index}\n",
    "\n",
    "    def get_num_batches(self, batch_size):\n",
    "        \"\"\" 배치 크기가 주어지면 데이터셋으로 만들 수 있는 배치 개수를 반환합니다\n",
    "        \n",
    "        매개변수:\n",
    "            batch_size (int)\n",
    "        반환값:\n",
    "            배치 개수\n",
    "        \"\"\"\n",
    "        return len(self) // batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caa17e2e",
   "metadata": {},
   "source": [
    "### 1. Vocabulary:\n",
    "    \n",
    "텍스트를 벡터릐 미니배치로 바꾸기: 토큰을 정수로 매핑하기\n",
    "    \n",
    "토큰과 정수 1대1 매핑: 2개의 딕셔너리 필요\n",
    "    \n",
    "두개의 딕셔너리를 Vocabulary 클래스에 캡슐화(데이터끼리 묶고 외부에는 안보여줌)\n",
    "\n",
    "Vocabulary 클래스는 딕셔너리에 사용자가 토큰 추가 시 자동으로 인덱스 증가 + UNK 특별토큰 관리\n",
    "\n",
    "자주 등장하지 않는 토큰 제한.\n",
    "\n",
    "#add_token(): Vocabulary에 새로운 토큰 추가\n",
    "#lookup_token(): 토큰에 해당하는 인덱스 추출\n",
    "#lookup_index(): 인덱스에 해당하는 토큰 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "067a0de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocabulary(object):\n",
    "    \"\"\" 매핑을 위해 텍스트를 처리하고 어휘 사전을 만드는 클래스 \"\"\"\n",
    "\n",
    "    def __init__(self, token_to_idx=None, add_unk=True, unk_token=\"<UNK>\"):\n",
    "        \"\"\"\n",
    "        매개변수:\n",
    "            token_to_idx (dict): 기존 토큰-인덱스 매핑 딕셔너리\n",
    "            add_unk (bool): UNK 토큰을 추가할지 지정하는 플래그\n",
    "            unk_token (str): Vocabulary에 추가할 UNK 토큰\n",
    "        \"\"\"\n",
    "\n",
    "        if token_to_idx is None:\n",
    "            token_to_idx = {}\n",
    "        self._token_to_idx = token_to_idx\n",
    "\n",
    "        self._idx_to_token = {idx: token \n",
    "                              for token, idx in self._token_to_idx.items()}\n",
    "        \n",
    "        self._add_unk = add_unk\n",
    "        self._unk_token = unk_token\n",
    "        \n",
    "        self.unk_index = -1\n",
    "        if add_unk:\n",
    "            self.unk_index = self.add_token(unk_token) \n",
    "        \n",
    "        \n",
    "    def to_serializable(self):\n",
    "        \"\"\" 직렬화할 수 있는 딕셔너리를 반환합니다 \"\"\"\n",
    "        return {'token_to_idx': self._token_to_idx, \n",
    "                'add_unk': self._add_unk, \n",
    "                'unk_token': self._unk_token}\n",
    "\n",
    "    @classmethod\n",
    "    def from_serializable(cls, contents):\n",
    "        \"\"\" 직렬화된 딕셔너리에서 Vocabulary 객체를 만듭니다 \"\"\"\n",
    "        return cls(**contents)\n",
    "\n",
    "    def add_token(self, token):\n",
    "        \"\"\" 토큰을 기반으로 매핑 딕셔너리를 업데이트합니다\n",
    "\n",
    "        매개변수:\n",
    "            token (str): Vocabulary에 추가할 토큰\n",
    "        반환값:\n",
    "            index (int): 토큰에 상응하는 정수\n",
    "        \"\"\"\n",
    "        if token in self._token_to_idx:\n",
    "            index = self._token_to_idx[token]\n",
    "        else:\n",
    "            index = len(self._token_to_idx)\n",
    "            self._token_to_idx[token] = index\n",
    "            self._idx_to_token[index] = token\n",
    "        return index\n",
    "    \n",
    "    def add_many(self, tokens):\n",
    "        \"\"\" 토큰 리스트를 Vocabulary에 추가합니다.\n",
    "        \n",
    "        매개변수:\n",
    "            tokens (list): 문자열 토큰 리스트\n",
    "        반환값:\n",
    "            indices (list): 토큰 리스트에 상응되는 인덱스 리스트\n",
    "        \"\"\"\n",
    "        return [self.add_token(token) for token in tokens]\n",
    "\n",
    "    def lookup_token(self, token):\n",
    "        \"\"\" 토큰에 대응하는 인덱스를 추출합니다.\n",
    "        토큰이 없으면 UNK 인덱스를 반환합니다.\n",
    "        \n",
    "        매개변수:\n",
    "            token (str): 찾을 토큰 \n",
    "        반환값:\n",
    "            index (int): 토큰에 해당하는 인덱스\n",
    "        노트:\n",
    "            UNK 토큰을 사용하려면 (Vocabulary에 추가하기 위해)\n",
    "            `unk_index`가 0보다 커야 합니다.\n",
    "        \"\"\"\n",
    "        if self.unk_index >= 0:\n",
    "            return self._token_to_idx.get(token, self.unk_index)\n",
    "        else:\n",
    "            return self._token_to_idx[token]\n",
    "\n",
    "    def lookup_index(self, index):\n",
    "        \"\"\" 인덱스에 해당하는 토큰을 반환합니다.\n",
    "        \n",
    "        매개변수: \n",
    "            index (int): 찾을 인덱스\n",
    "        반환값:\n",
    "            token (str): 인텍스에 해당하는 토큰\n",
    "        에러:\n",
    "            KeyError: 인덱스가 Vocabulary에 없을 때 발생합니다.\n",
    "        \"\"\"\n",
    "        if index not in self._idx_to_token:\n",
    "            raise KeyError(\"Vocabulary에 인덱스(%d)가 없습니다.\" % index)\n",
    "        return self._idx_to_token[index]\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"<Vocabulary(size=%d)>\" % len(self)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._token_to_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "666a6733",
   "metadata": {},
   "source": [
    "### 2. Vecotrizer\n",
    "\n",
    "입력데이터 포인트의 토큰을 순회하며 각 토큰을 정수로 바꾸기\n",
    "\n",
    "해당 클래스 결과는 항상 벡터이고 Vectorizer에서 만든 벡터는 항상 길이가 같다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2b9dc368",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReviewVectorizer(object):\n",
    "    \"\"\" 어휘 사전을 생성하고 관리합니다 \"\"\"\n",
    "    def __init__(self, review_vocab, rating_vocab):\n",
    "        \"\"\"\n",
    "        매개변수:\n",
    "            review_vocab (Vocabulary): 단어를 정수에 매핑하는 Vocabulary\n",
    "            rating_vocab (Vocabulary): 클래스 레이블을 정수에 매핑하는 Vocabulary\n",
    "        \"\"\"\n",
    "        self.review_vocab = review_vocab\n",
    "        self.rating_vocab = rating_vocab\n",
    "\n",
    "    def vectorize(self, review):\n",
    "        \"\"\" 리뷰에 대한 웟-핫 벡터를 만듭니다\n",
    "        \n",
    "        매개변수:\n",
    "            review (str): 리뷰\n",
    "        반환값:\n",
    "            one_hot (np.ndarray): 원-핫 벡터\n",
    "        \"\"\"\n",
    "        one_hot = np.zeros(len(self.review_vocab), dtype=np.float32)\n",
    "        \n",
    "        for token in review.split(\" \"):\n",
    "            if token not in string.punctuation:\n",
    "                one_hot[self.review_vocab.lookup_token(token)] = 1\n",
    "\n",
    "        return one_hot\n",
    "\n",
    "    #이 지점이 Vecotrizer 클래스 초기화하는 집입점\n",
    "    #from_dataframe(): 1. 데이터셋에 있는 모든 빈도수 카운트, 2. 키워드 매개변수 cutoff에 지정한 수보다 빈도 높은 토큰만 사용하는 객체 생성\n",
    "    #리뷰의 단어에 해당하는 위치가 1\n",
    "    @classmethod\n",
    "    def from_dataframe(cls, review_df, cutoff=25):\n",
    "        \"\"\" 데이터셋 데이터프레임에서 Vectorizer 객체를 만듭니다\n",
    "        \n",
    "        매개변수:\n",
    "            review_df (pandas.DataFrame): 리뷰 데이터셋\n",
    "            cutoff (int): 빈도 기반 필터링 설정값\n",
    "        반환값:\n",
    "            ReviewVectorizer 객체\n",
    "        \"\"\"\n",
    "        review_vocab = Vocabulary(add_unk=True)\n",
    "        rating_vocab = Vocabulary(add_unk=False)\n",
    "        \n",
    "        # 점수를 추가합니다\n",
    "        for rating in sorted(set(review_df.rating)):\n",
    "            rating_vocab.add_token(rating)\n",
    "\n",
    "        # count > cutoff인 단어를 추가합니다\n",
    "        word_counts = Counter()\n",
    "        for review in review_df.review:\n",
    "            for word in review.split(\" \"):\n",
    "                if word not in string.punctuation:\n",
    "                    word_counts[word] += 1\n",
    "               \n",
    "        for word, count in word_counts.items():\n",
    "            if count > cutoff:\n",
    "                review_vocab.add_token(word)\n",
    "\n",
    "        return cls(review_vocab, rating_vocab)\n",
    "\n",
    "    @classmethod\n",
    "    def from_serializable(cls, contents):\n",
    "        \"\"\" 직렬화된 딕셔너리에서 ReviewVectorizer 객체를 만듭니다\n",
    "        \n",
    "        매개변수:\n",
    "            contents (dict): 직렬화된 딕셔너리\n",
    "        반환값:\n",
    "            ReviewVectorizer 클래스 객체\n",
    "        \"\"\"\n",
    "        review_vocab = Vocabulary.from_serializable(contents['review_vocab'])\n",
    "        rating_vocab =  Vocabulary.from_serializable(contents['rating_vocab'])\n",
    "\n",
    "        return cls(review_vocab=review_vocab, rating_vocab=rating_vocab)\n",
    "\n",
    "    def to_serializable(self):\n",
    "        \"\"\" 캐싱을 위해 직렬화된 딕셔너리를 만듭니다\n",
    "        \n",
    "        반환값:\n",
    "            contents (dict): 직렬화된 딕셔너리\n",
    "        \"\"\"\n",
    "        return {'review_vocab': self.review_vocab.to_serializable(),\n",
    "                'rating_vocab': self.rating_vocab.to_serializable()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86ddd2d8",
   "metadata": {},
   "source": [
    "제약 2가지 존재\n",
    "\n",
    "1. 희소배열(한 리뷰의 고유 단어 수는 항상 Vocabulary 전체 단어 수보다 적다)\n",
    "2. 리뷰에 등장하는 단어 순서 무시"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d351b9e",
   "metadata": {},
   "source": [
    "### 3. DataLoader\n",
    "\n",
    "미니배치로 모으는 작업 편하게 해줌"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "15e9733e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batches(dataset, batch_size, shuffle=True,\n",
    "                     drop_last=True, device=\"cpu\"):\n",
    "    \"\"\"\n",
    "    파이토치 DataLoader를 감싸고 있는 제너레이터 함수.\n",
    "    걱 텐서를 지정된 장치로 이동합니다.\n",
    "    \"\"\"\n",
    "    dataloader = DataLoader(dataset=dataset, batch_size=batch_size,\n",
    "                            shuffle=shuffle, drop_last=drop_last)\n",
    "\n",
    "    for data_dict in dataloader:\n",
    "        out_data_dict = {}\n",
    "        for name, tensor in data_dict.items():\n",
    "            out_data_dict[name] = data_dict[name].to(device)\n",
    "        yield out_data_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c942761",
   "metadata": {},
   "source": [
    "# 분류모델 만들기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b99c14d",
   "metadata": {},
   "source": [
    "퍼셉트론 기반 분류기\n",
    "\n",
    "시그모이드 없이 수치적으로 안정된 계산을 위한 BCEWithLogitsLoss()로 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6dae5364",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class ReviewClassifier(nn.Module):\n",
    "    \"\"\" 간단한 퍼셉트론 기반 분류기 \"\"\"\n",
    "    def __init__(self, num_features):\n",
    "        \"\"\"\n",
    "        매개변수:\n",
    "            num_features (int): 입력 특성 벡트의 크기\n",
    "        \"\"\"\n",
    "        super(ReviewClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(in_features=num_features, \n",
    "                             out_features=1)\n",
    "\n",
    "    def forward(self, x_in, apply_sigmoid=False):\n",
    "        \"\"\" 분류기의 정방향 계산\n",
    "        \n",
    "        매개변수:\n",
    "            x_in (torch.Tensor): 입력 데이터 텐서 \n",
    "                x_in.shape는 (batch, num_features)입니다.\n",
    "            apply_sigmoid (bool): 시그모이드 활성화 함수를 위한 플래그\n",
    "                크로스-엔트로피 손실을 사용하려면 False로 지정합니다\n",
    "        반환값:\n",
    "            결과 텐서. tensor.shape은 (batch,)입니다.\n",
    "        \"\"\"\n",
    "        y_out = self.fc1(x_in).squeeze()\n",
    "        #forward는 기본값에서 시그모이드 적용하지 않지만 확률값 알기 위해서 옵션으로 빼둔다\n",
    "        if apply_sigmoid:\n",
    "            y_out = torch.sigmoid(y_out)\n",
    "        return y_out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e748f90c",
   "metadata": {},
   "source": [
    "### 모델 훈련"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1608870e",
   "metadata": {},
   "source": [
    "모델 만들기 >> 데이터셋 순회 >> 입력데이터에서 모델의 출력 게산 >> 손실함수 계산 >> 모델 수정"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "686263cc",
   "metadata": {},
   "source": [
    "args 내에 사용할 변수 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6be335e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "파일 경로: \n",
      "\t/Users/jihyeonbin/model_storage/vectorizer.json\n",
      "\t/Users/jihyeonbin/model_storage/model.pth\n",
      "CUDA 사용여부: False\n"
     ]
    }
   ],
   "source": [
    "args = Namespace(\n",
    "    # 날짜와 경로 정보\n",
    "    frequency_cutoff=25, #25번 이상 출현한 단어만 학습할 때\n",
    "    model_state_file='model.pth',\n",
    "    review_csv='/Users/jihyeonbin/reviews_with_splits_lite.csv',\n",
    "    # review_csv='data/yelp/reviews_with_splits_full.csv'(전처리마친 csv 위치),\n",
    "    save_dir='/Users/jihyeonbin/model_storage/',\n",
    "    vectorizer_file='vectorizer.json',#vecotrizer 저장할 파일 이름\n",
    "    # 모델 하이퍼파라미터 없음\n",
    "    # 훈련 하이퍼파라미터\n",
    "    batch_size=128,\n",
    "    early_stopping_criteria=5,\n",
    "    learning_rate=0.001,\n",
    "    num_epochs=100,\n",
    "    seed=1337,\n",
    "    # 실행 옵션\n",
    "    catch_keyboard_interrupt=True,\n",
    "    cuda=True,\n",
    "    expand_filepaths_to_save_dir=True,\n",
    "    reload_from_files=False,\n",
    ")\n",
    "\n",
    "if args.expand_filepaths_to_save_dir:\n",
    "    args.vectorizer_file = os.path.join(args.save_dir,\n",
    "                                        args.vectorizer_file)\n",
    "\n",
    "    args.model_state_file = os.path.join(args.save_dir,\n",
    "                                         args.model_state_file)\n",
    "    \n",
    "    print(\"파일 경로: \")\n",
    "    print(\"\\t{}\".format(args.vectorizer_file))\n",
    "    print(\"\\t{}\".format(args.model_state_file))\n",
    "    \n",
    "# CUDA 체크\n",
    "if not torch.cuda.is_available():\n",
    "    args.cuda = False\n",
    "\n",
    "print(\"CUDA 사용여부: {}\".format(args.cuda))\n",
    "\n",
    "args.device = torch.device(\"cuda\" if args.cuda else \"cpu\")\n",
    "\n",
    "# 재현성을 위해 시드 설정\n",
    "set_seed_everywhere(args.seed, args.cuda)\n",
    "\n",
    "# 디렉토리 처리\n",
    "handle_dirs(args.save_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6b3e545",
   "metadata": {},
   "source": [
    "### 훈련 준비"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b99b38",
   "metadata": {},
   "source": [
    "데이터셋, 모델, 손실, 옵티마이저, 훈련상태 딕셔너리 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c871dcd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "def make_train_state(args): #변수들 저장하는 공간 만듬\n",
    "    return {'stop_early': False,\n",
    "            'early_stopping_step': 0,\n",
    "            'early_stopping_best_val': 1e8,\n",
    "            'learning_rate': args.learning_rate,\n",
    "            'epoch_index': 0,\n",
    "            'train_loss': [],\n",
    "            'train_acc': [],\n",
    "            'val_loss': [],\n",
    "            'val_acc': [],\n",
    "            'test_loss': -1,\n",
    "            'test_acc': -1,\n",
    "            'model_filename': args.model_state_file}\n",
    "\n",
    "def update_train_state(args, model, train_state):\n",
    "    \"\"\" 훈련 상태를 업데이트합니다.\n",
    "\n",
    "    Components:\n",
    "     - 조기 종료: 과대 적합 방지\n",
    "     - 모델 체크포인트: 더 나은 모델을 저장합니다\n",
    "\n",
    "    :param args: 메인 매개변수\n",
    "    :param model: 훈련할 모델\n",
    "    :param train_state: 훈련 상태를 담은 딕셔너리\n",
    "    :returns:\n",
    "        새로운 훈련 상태\n",
    "    \"\"\"\n",
    "\n",
    "    # 적어도 한 번 모델을 저장합니다\n",
    "    if train_state['epoch_index'] == 0:\n",
    "        torch.save(model.state_dict(), train_state['model_filename'])\n",
    "        train_state['stop_early'] = False\n",
    "\n",
    "    # 성능이 향상되면 모델을 저장합니다\n",
    "    elif train_state['epoch_index'] >= 1:\n",
    "        loss_tm1, loss_t = train_state['val_loss'][-2:]\n",
    "\n",
    "        # 손실이 나빠지면\n",
    "        if loss_t >= train_state['early_stopping_best_val']:\n",
    "            # 조기 종료 단계 업데이트\n",
    "            train_state['early_stopping_step'] += 1\n",
    "        # 손실이 감소하면\n",
    "        else:\n",
    "            # 최상의 모델 저장\n",
    "            if loss_t < train_state['early_stopping_best_val']:\n",
    "                torch.save(model.state_dict(), train_state['model_filename'])\n",
    "\n",
    "            # 조기 종료 단계 재설정\n",
    "            train_state['early_stopping_step'] = 0\n",
    "\n",
    "        # 조기 종료 여부 확인\n",
    "        train_state['stop_early'] = \\\n",
    "            train_state['early_stopping_step'] >= args.early_stopping_criteria\n",
    "\n",
    "    return train_state\n",
    "\n",
    "def compute_accuracy(y_pred, y_target): #정확도 계산\n",
    "    y_target = y_target.cpu()\n",
    "    y_pred_indices = (torch.sigmoid(y_pred)>0.5).cpu().long()#.max(dim=1)[1]\n",
    "    n_correct = torch.eq(y_pred_indices, y_target).sum().item()\n",
    "    return n_correct / len(y_pred_indices) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78803118",
   "metadata": {},
   "source": [
    "### 초기화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "431e9290",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: command not found: wget\n",
      "zsh:1: command not found: wget\n",
      "chmod: data/get-all-data.sh: No such file or directory\n",
      "/Users/jihyeonbin/data\n",
      "zsh:1: no such file or directory: ./get-all-data.sh\n",
      "/Users/jihyeonbin\n"
     ]
    }
   ],
   "source": [
    "!mkdir data\n",
    "!wget https://git.io/JtRSq -O data/download.py\n",
    "!wget https://git.io/JtRSO -O data/get-all-data.sh\n",
    "!chmod 755 data/get-all-data.sh\n",
    "%cd data\n",
    "!./get-all-data.sh\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2be5d775",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터셋을 로드하고 Vectorizer를 만듭니다\n"
     ]
    }
   ],
   "source": [
    "if args.reload_from_files: #원래 파일이 있으면\n",
    "    # 체크포인트에서 훈련을 다시 시작\n",
    "    print(\"데이터셋과 Vectorizer를 로드합니다\")\n",
    "    dataset = ReviewDataset.load_dataset_and_load_vectorizer(args.review_csv,\n",
    "                                                            args.vectorizer_file)\n",
    "else:\n",
    "    print(\"데이터셋을 로드하고 Vectorizer를 만듭니다\")\n",
    "    # 데이터셋과 Vectorizer 만들기\n",
    "    dataset = ReviewDataset.load_dataset_and_make_vectorizer(args.review_csv)\n",
    "    dataset.save_vectorizer(args.vectorizer_file)    \n",
    "vectorizer = dataset.get_vectorizer()\n",
    "\n",
    "classifier = ReviewClassifier(num_features=len(vectorizer.review_vocab))\n",
    "classifier = classifier.to(args.device)\n",
    "\n",
    "loss_func = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(classifier.parameters(), lr=args.learning_rate) #Adam 사용\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer,\n",
    "                                                 mode='min', factor=0.5,\n",
    "                                                 patience=1) #learningrate 조정 가능\n",
    "train_state = make_train_state(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a34f21d",
   "metadata": {},
   "source": [
    "### 훈련 반복\n",
    "\n",
    "tqpm 사용: 진행률 프로세스바"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fc2f610",
   "metadata": {},
   "source": [
    "2개의 반복문으로 구성\n",
    "\n",
    "내부 반복문: 데이터셋의 미니배치에 대해서 반복 수행\n",
    "    \n",
    "외부 분복문: 내부 반복문 여러번 반복\n",
    "    \n",
    "내부 반복문에서 미니배치마다 손실을 계산하고 옵티마이저가 모델 파라미터를 업데이트한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "140bd6ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba8f172375b84b47b4bca8c3758f3474",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training routine:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea93a3846bec473eaad256595b488523",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "split=train:   0%|          | 0/306 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48688cd48119403ebef6285a935f009f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "split=val:   0%|          | 0/65 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "epoch_bar = tqdm.notebook.tqdm(desc='training routine', \n",
    "                          total=args.num_epochs,\n",
    "                          position=0)\n",
    "\n",
    "dataset.set_split('train')\n",
    "train_bar = tqdm.notebook.tqdm(desc='split=train',\n",
    "                          total=dataset.get_num_batches(args.batch_size), \n",
    "                          position=1, \n",
    "                          leave=True)\n",
    "dataset.set_split('val')\n",
    "val_bar = tqdm.notebook.tqdm(desc='split=val',\n",
    "                        total=dataset.get_num_batches(args.batch_size), \n",
    "                        position=1, \n",
    "                        leave=True)\n",
    "\n",
    "try:\n",
    "    for epoch_index in range(args.num_epochs):\n",
    "        train_state['epoch_index'] = epoch_index\n",
    "        #현 에포크 인덱스 저장\n",
    "\n",
    "        # 훈련 세트에 대한 순회\n",
    "\n",
    "        # 훈련 세트와 배치 제너레이터 준비, 손실과 정확도를 0으로 설정\n",
    "        dataset.set_split('train')\n",
    "        batch_generator = generate_batches(dataset, \n",
    "                                           batch_size=args.batch_size, \n",
    "                                           device=args.device)\n",
    "        running_loss = 0.0\n",
    "        running_acc = 0.0\n",
    "        classifier.train()\n",
    "\n",
    "        for batch_index, batch_dict in enumerate(batch_generator):\n",
    "            # 훈련 과정은 5단계로 이루어집니다\n",
    "\n",
    "            # --------------------------------------\n",
    "            # 단계 1. 그레이디언트를 0으로 초기화합니다\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # 단계 2. 출력을 계산합니다\n",
    "            y_pred = classifier(x_in=batch_dict['x_data'].float())\n",
    "\n",
    "            # 단계 3. 손실을 계산합니다\n",
    "            loss = loss_func(y_pred, batch_dict['y_target'].float())\n",
    "            loss_t = loss.item()\n",
    "            running_loss += (loss_t - running_loss) / (batch_index + 1)\n",
    "\n",
    "            # 단계 4. 손실을 사용해 그레이디언트를 계산합니다\n",
    "            loss.backward()\n",
    "\n",
    "            # 단계 5. 옵티마이저로 가중치를 업데이트합니다\n",
    "            optimizer.step()\n",
    "            # -----------------------------------------\n",
    "            \n",
    "            # 정확도를 계산합니다\n",
    "            acc_t = compute_accuracy(y_pred, batch_dict['y_target'])\n",
    "            running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
    "\n",
    "            # 진행 바 업데이트\n",
    "            train_bar.set_postfix(loss=running_loss, \n",
    "                                  acc=running_acc, \n",
    "                                  epoch=epoch_index)\n",
    "            train_bar.update()\n",
    "\n",
    "        train_state['train_loss'].append(running_loss)\n",
    "        train_state['train_acc'].append(running_acc)\n",
    "\n",
    "        # 검증 세트에 대한 순회\n",
    "\n",
    "        # 검증 세트와 배치 제너레이터 준비, 손실과 정확도를 0으로 설정\n",
    "        dataset.set_split('val')\n",
    "        batch_generator = generate_batches(dataset, \n",
    "                                           batch_size=args.batch_size, \n",
    "                                           device=args.device)\n",
    "        running_loss = 0.\n",
    "        running_acc = 0.\n",
    "        classifier.eval()\n",
    "\n",
    "        for batch_index, batch_dict in enumerate(batch_generator):\n",
    "\n",
    "            # 단계 1. 출력을 계산합니다\n",
    "            y_pred = classifier(x_in=batch_dict['x_data'].float())\n",
    "\n",
    "            # 단계 2. 손실을 계산합니다\n",
    "            loss = loss_func(y_pred, batch_dict['y_target'].float())\n",
    "            loss_t = loss.item()\n",
    "            running_loss += (loss_t - running_loss) / (batch_index + 1)\n",
    "\n",
    "            # 단계 3. 정확도를 계산합니다\n",
    "            acc_t = compute_accuracy(y_pred, batch_dict['y_target'])\n",
    "            running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
    "            \n",
    "            val_bar.set_postfix(loss=running_loss, \n",
    "                                acc=running_acc, \n",
    "                                epoch=epoch_index)\n",
    "            val_bar.update()\n",
    "\n",
    "        train_state['val_loss'].append(running_loss)\n",
    "        train_state['val_acc'].append(running_acc)\n",
    "\n",
    "        train_state = update_train_state(args=args, model=classifier,\n",
    "                                         train_state=train_state)\n",
    "\n",
    "        scheduler.step(train_state['val_loss'][-1])\n",
    "\n",
    "        train_bar.n = 0\n",
    "        val_bar.n = 0\n",
    "        epoch_bar.update()\n",
    "\n",
    "        if train_state['stop_early']:\n",
    "            break\n",
    "\n",
    "        train_bar.n = 0\n",
    "        val_bar.n = 0\n",
    "        epoch_bar.update()\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Exiting loop\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d8cd1ce",
   "metadata": {},
   "source": [
    "가장 좋은 모델 활용해 loss와 정확도 계산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e425477a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 가장 좋은 모델을 사용해 테스트 세트의 손실과 정확도를 계산합니다\n",
    "classifier.load_state_dict(torch.load(train_state['model_filename']))\n",
    "classifier = classifier.to(args.device)\n",
    "\n",
    "dataset.set_split('test')\n",
    "batch_generator = generate_batches(dataset, \n",
    "                                   batch_size=args.batch_size, \n",
    "                                   device=args.device)\n",
    "running_loss = 0.\n",
    "running_acc = 0.\n",
    "classifier.eval()\n",
    "\n",
    "for batch_index, batch_dict in enumerate(batch_generator):\n",
    "    # 출력을 계산합니다\n",
    "    y_pred = classifier(x_in=batch_dict['x_data'].float())\n",
    "\n",
    "    # 손실을 계산합니다\n",
    "    loss = loss_func(y_pred, batch_dict['y_target'].float())\n",
    "    loss_t = loss.item()\n",
    "    running_loss += (loss_t - running_loss) / (batch_index + 1)\n",
    "\n",
    "    # 정확도를 계산합니다\n",
    "    acc_t = compute_accuracy(y_pred, batch_dict['y_target'])\n",
    "    running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
    "\n",
    "train_state['test_loss'] = running_loss\n",
    "train_state['test_acc'] = running_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "7e7221bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "테스트 손실: 0.216\n",
      "테스트 정확도: 91.899\n"
     ]
    }
   ],
   "source": [
    "print(\"테스트 손실: {:.3f}\".format(train_state['test_loss']))\n",
    "print(\"테스트 정확도: {:.3f}\".format(train_state['test_acc']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80d526be",
   "metadata": {},
   "source": [
    "### 새로운 데이터포인트 추론하여 분류하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "cdc5c4e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"([.,!?])\", r\" \\1 \", text)\n",
    "    text = re.sub(r\"[^a-zA-Z.,!?]+\", r\" \", text)\n",
    "    return text\n",
    "\n",
    "#이상한거 날리기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "68e4ec0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_rating(review, classifier, vectorizer, decision_threshold=0.5):\n",
    "    \"\"\" 리뷰 점수 예측하기\n",
    "    \n",
    "    매개변수:\n",
    "        review (str): 리뷰 텍스트\n",
    "        classifier (ReviewClassifier): 훈련된 모델\n",
    "        vectorizer (ReviewVectorizer): Vectorizer 객체\n",
    "        decision_threshold (float): 클래스를 나눌 결정 경계\n",
    "    \"\"\"\n",
    "    review = preprocess_text(review)\n",
    "    \n",
    "    vectorized_review = torch.tensor(vectorizer.vectorize(review))\n",
    "    result = classifier(vectorized_review.view(1, -1))\n",
    "    \n",
    "    probability_value = torch.sigmoid(result).item()\n",
    "    index = 1\n",
    "    if probability_value < decision_threshold:\n",
    "        index = 0\n",
    "\n",
    "    return vectorizer.rating_vocab.lookup_index(index)\n",
    "\n",
    "#threshold: 경계값 기준"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "91ff5e8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is a pretty awesome book -> positive\n"
     ]
    }
   ],
   "source": [
    "test_review = \"this is a pretty awesome book\"\n",
    "\n",
    "classifier = classifier.cpu()\n",
    "prediction = predict_rating(test_review, classifier, vectorizer, decision_threshold=0.5)\n",
    "print(\"{} -> {}\".format(test_review, prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d48997b",
   "metadata": {},
   "source": [
    "### 모델 가중치 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "0b43ba59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 7326])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.fc1.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "d0be9b7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "긍정 리뷰에 영향을 미치는 단어:\n",
      "--------------------------------------\n",
      "delicious\n",
      "pleasantly\n",
      "fantastic\n",
      "amazing\n",
      "vegas\n",
      "great\n",
      "ngreat\n",
      "yum\n",
      "excellent\n",
      "perfect\n",
      "awesome\n",
      "yummy\n",
      "love\n",
      "bomb\n",
      "chinatown\n",
      "deliciousness\n",
      "solid\n",
      "pleased\n",
      "notch\n",
      "hooked\n",
      "====\n",
      "\n",
      "\n",
      "\n",
      "부정 리뷰에 영향을 미치는 단어:\n",
      "--------------------------------------\n",
      "worst\n",
      "mediocre\n",
      "bland\n",
      "horrible\n",
      "meh\n",
      "awful\n",
      "rude\n",
      "terrible\n",
      "tasteless\n",
      "overpriced\n",
      "unacceptable\n",
      "disgusting\n",
      "slowest\n",
      "poorly\n",
      "unfriendly\n",
      "nmaybe\n",
      "underwhelmed\n",
      "disappointing\n",
      "disappointment\n",
      "downhill\n"
     ]
    }
   ],
   "source": [
    "# 가중치 정렬\n",
    "fc1_weights = classifier.fc1.weight.detach()[0]\n",
    "_, indices = torch.sort(fc1_weights, dim=0, descending=True)\n",
    "indices = indices.numpy().tolist()\n",
    "\n",
    "# 긍정적인 상위 20개 단어\n",
    "print(\"긍정 리뷰에 영향을 미치는 단어:\")\n",
    "print(\"--------------------------------------\")\n",
    "for i in range(20):\n",
    "    print(vectorizer.review_vocab.lookup_index(indices[i]))\n",
    "    \n",
    "print(\"====\\n\\n\\n\")\n",
    "\n",
    "# 부정적인 상위 20개 단어\n",
    "print(\"부정 리뷰에 영향을 미치는 단어:\")\n",
    "print(\"--------------------------------------\")\n",
    "indices.reverse()\n",
    "for i in range(20):\n",
    "    print(vectorizer.review_vocab.lookup_index(indices[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4ebd1dc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
